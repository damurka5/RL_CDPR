{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "import gym\n",
    "import warnings\n",
    "from torch.distributions.categorical import Categorical\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from robot_model import *\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "\n",
    "class CDPR4_env(CDPR4):\n",
    "    def __init__(self, start_state=np.array([.0, .0, 1.0, .0, .0, .0]), desired_state=np.array([.0, .0, 2.0, .0, .0, .0]), pos=np.array([.0, .0, 1.0]), params=params, approx=1, mass=1):\n",
    "        super().__init__(pos=np.array([.0, .0, 1.]), params=params, approx=1, mass=1)\n",
    "\n",
    "        self.start_state = start_state.copy()  # start position 1m on Z\n",
    "        self.cur_state = np.array([.0, .0, 1., .0, .0, .0]) # X=[pos, vel] in control\n",
    "        self.reward = 0 # reward 0 is 0 error in position, closer position to desired -> higher reward \n",
    "        self.desired_state = desired_state\n",
    "        self.v = np.array([.0, .0, .0])\n",
    "        self.control = np.array([.0, .0, .0, .0])\n",
    "    \n",
    "        self.max_speed = 10\n",
    "        self.max_force = 15 \n",
    "        self.dt = 0.1\n",
    "        \n",
    "        self.action_space = spaces.MultiDiscrete(np.array([[2], [2], [2], [2]])) # 1 - pull, 0 - release\n",
    "        self.observation_space = spaces.Box(low=np.array([-1.154, -1.404, .0, -self.max_speed, -self.max_speed, -self.max_speed]), \n",
    "                                            high=np.array([1.154, 1.404, 3.220,  self.max_speed, self.max_speed, self.max_speed]))\n",
    "        self._max_episode_steps = 500 # default in gymnasium env is 1000\n",
    "        self._elapsed_steps = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        state = self.observation_space.sample()\n",
    "        \n",
    "        # self.reward = -1 # 1 meter away from desired position\n",
    "        self._elapsed_steps = 0\n",
    "        \n",
    "        self.pos = state[:3].flatten()\n",
    "        self.v = state[3:].flatten()\n",
    "        \n",
    "        self.reward = -np.linalg.norm(self.pos - self.desired_state[:3].flatten())\n",
    "        \n",
    "        return state, {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        pos = self.cur_state[:3].flatten()\n",
    "        vel = self.cur_state[3:].flatten()\n",
    "        m = self.m\n",
    "\n",
    "        dt = self.dt\n",
    "        u = self.max_force*action.reshape((4,1)).cpu().detach().numpy()\n",
    "        costs = np.linalg.norm(pos - self.desired_state[:3].flatten()) # reward function includes only position, no velocities\n",
    "        costs += 0.5*np.linalg.norm(vel - self.desired_state[3:].flatten()) # velocity factor\n",
    "        # print(f'cost {costs}')\n",
    "        \n",
    "        dXdt = self.B() @ u + np.array([.0, .0, .0, .0, .0, -g]).reshape((6,1))\n",
    "        # print(self.B() @ u)\n",
    "        new_vel = vel + dXdt[:3].flatten()*dt\n",
    "        new_pos = pos + vel*dt + 0.5*dXdt[3:].flatten()*dt**2\n",
    "\n",
    "        self.pos = new_pos\n",
    "        self.v = new_vel\n",
    "\n",
    "        state = np.hstack((new_pos, new_vel))\n",
    "        self.cur_state = state\n",
    "        \n",
    "        terminated = np.allclose(self.cur_state, self.desired_state, atol=1e-03) # reached desired position\n",
    "        \n",
    "        self._elapsed_steps += 1\n",
    "\n",
    "        truncated = False\n",
    "        if self._elapsed_steps >= self._max_episode_steps:\n",
    "            truncated = True\n",
    "        \n",
    "        return state, -costs, terminated, truncated, {} #\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory():\n",
    "    \"\"\"\n",
    "    Memory for PPO\n",
    "    \"\"\"\n",
    "    def  __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.actions= []\n",
    "        self.action_probs = []\n",
    "        self.rewards = []\n",
    "        self.vals = []\n",
    "        self.dones = []\n",
    "        \n",
    "        self. batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        ## suppose n_states=20 and batch_size = 4\n",
    "        n_states = len(self.states)\n",
    "        ##n_states should be always greater than batch_size\n",
    "        ## batch_start is the starting index of every batch\n",
    "        ## eg:   array([ 0,  4,  8, 12, 16]))\n",
    "        batch_start = np.arange(0, n_states, self.batch_size) \n",
    "        ## random shuffling if indexes\n",
    "        # eg: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        ## eg: array([12, 17,  6,  7, 10, 11, 15, 13, 18,  9,  8,  4,  3,  0,  2,  5, 14,19,  1, 16])\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "        ## eg: [array([12, 17,  6,  7]),array([10, 11, 15, 13]),array([18,  9,  8,  4]),array([3, 0, 2, 5]),array([14, 19,  1, 16])]\n",
    "        \n",
    "        states = np.array([self.states[0]])\n",
    "        actions = self.actions[0].cpu().detach().numpy()\n",
    "        action_probs = self.action_probs[0].cpu().detach().numpy()\n",
    "        \n",
    "        return states,actions,\\\n",
    "               action_probs,np.array(self.vals),np.array(self.rewards),\\\n",
    "               np.array(self.dones),batches\n",
    "\n",
    "    def store_memory(self,state,action,action_prob,val,reward,done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.action_probs.append(action_prob)\n",
    "        self.rewards.append(reward)\n",
    "        self.vals.append(val)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.actions= []\n",
    "        self.action_probs = []\n",
    "        self.rewards = []\n",
    "        self.vals = []\n",
    "        self.dones = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNwk(nn.Module):\n",
    "    def __init__(self,input_dim,out_dim,\n",
    "                 adam_lr,\n",
    "                 chekpoint_file,\n",
    "                 hidden1_dim=256,\n",
    "                 hidden2_dim=256\n",
    "                 ):\n",
    "        super(ActorNwk, self).__init__()\n",
    "\n",
    "        self.actor_nwk = nn.Sequential(\n",
    "            nn.Linear(input_dim,hidden1_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1_dim,hidden2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2_dim,out_dim),  \n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.checkpoint_file = chekpoint_file\n",
    "        self.optimizer = torch.optim.Adam(params=self.actor_nwk.parameters(),lr=adam_lr)\n",
    "\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    \n",
    "    def forward(self,state):\n",
    "        out = self.actor_nwk(state)\n",
    "        dist = Categorical(out.reshape((4,1)))\n",
    "        return dist\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNwk(nn.Module):\n",
    "    def __init__(self,input_dim,\n",
    "                 adam_lr,\n",
    "                 chekpoint_file,\n",
    "                 hidden1_dim=256,\n",
    "                 hidden2_dim=256\n",
    "                 ):\n",
    "        super(CriticNwk, self).__init__()\n",
    "\n",
    "        self.critic_nwk = nn.Sequential(\n",
    "            nn.Linear(input_dim,hidden1_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1_dim,hidden2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2_dim,1),  \n",
    "   \n",
    "        )\n",
    "\n",
    "        self.checkpoint_file = chekpoint_file\n",
    "        self.optimizer = torch.optim.Adam(params=self.critic_nwk.parameters(),lr=adam_lr)\n",
    "\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    \n",
    "    def forward(self,state):\n",
    "        out = self.critic_nwk(state)\n",
    "        return out\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, gamma, policy_clip,lamda, adam_lr,\n",
    "                 n_epochs, batch_size, state_dim, action_dim):\n",
    "        \n",
    "        self.gamma = gamma \n",
    "        self.policy_clip = policy_clip\n",
    "        self.lamda  = lamda\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        self.actor = ActorNwk(input_dim=state_dim,out_dim=action_dim,adam_lr=adam_lr,chekpoint_file='tmp/actor')\n",
    "        self.critic = CriticNwk(input_dim=state_dim,adam_lr=adam_lr,chekpoint_file='tmp/ctitic')\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "\n",
    "    def store_data(self,state,action,action_prob,val,reward,done):\n",
    "        self.memory.store_memory(state,action,action_prob,val,reward,done)\n",
    "       \n",
    "\n",
    "    def save_models(self):\n",
    "        print('... Saving Models ......')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "    \n",
    "    def load_models(self):\n",
    "        print('... Loading models ...')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.actor.device)\n",
    "        # print(f'state {state}')\n",
    "        dist = self.actor(state)\n",
    "        # print(f'dist {dist}')\n",
    "        ## sample the output action from a categorical distribution of predicted actions\n",
    "        action = dist.sample()\n",
    "        # print(f'action sampled {action}')\n",
    "        probs = torch.squeeze(dist.log_prob(action))\n",
    "        # print(f'probs {probs}')\n",
    "        action = torch.squeeze(action)\n",
    "        # print(f'action  {action}')\n",
    "\n",
    "        ## value from critic model\n",
    "        value = self.critic(state)\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "    \n",
    "    def calculate_advanatage(self,reward_arr,value_arr,dones_arr):\n",
    "        time_steps = len(reward_arr)\n",
    "        advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "        for t in range(0,time_steps-1):\n",
    "            discount = 1\n",
    "            running_advantage = 0\n",
    "            for k in range(t,time_steps-1):\n",
    "                if int(dones_arr[k]) == 1:\n",
    "                    running_advantage += reward_arr[k] - value_arr[k]\n",
    "                else:\n",
    "                \n",
    "                    running_advantage += reward_arr[k] + (self.gamma*value_arr[k+1]) - value_arr[k]\n",
    "\n",
    "                running_advantage = discount * running_advantage\n",
    "                # running_advantage += discount*(reward_arr[k] + self.gamma*value_arr[k+1]*(1-int(dones_arr[k])) - value_arr[k])\n",
    "                discount *= self.gamma * self.lamda\n",
    "            \n",
    "            advantage[t] = running_advantage\n",
    "        advantage = torch.tensor(advantage).to(self.actor.device)\n",
    "        return advantage\n",
    "    \n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "\n",
    "            ## initially all will be empty arrays\n",
    "            state_arr, action_arr, old_prob_arr, value_arr,\\\n",
    "            reward_arr, dones_arr, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "            \n",
    "            advantage_arr = self.calculate_advanatage(reward_arr,value_arr,dones_arr)\n",
    "            values = torch.tensor(value_arr).to(self.actor.device)\n",
    "\n",
    "            for batch in batches:\n",
    "                states = torch.tensor(state_arr[batch], dtype=torch.float).to(self.actor.device)\n",
    "                old_probs = torch.tensor(old_prob_arr[batch]).to(self.actor.device)\n",
    "                actions = torch.tensor(action_arr[batch]).to(self.actor.device)\n",
    "                dist = self.actor(states)\n",
    "                critic_value = self.critic(states)\n",
    "\n",
    "                critic_value = torch.squeeze(critic_value)\n",
    "\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "                #prob_ratio = (new_probs - old_probs).exp()\n",
    "                weighted_probs = advantage_arr[batch] * prob_ratio\n",
    "                weighted_clipped_probs = torch.clamp(prob_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage_arr[batch]\n",
    "                actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                returns = advantage_arr[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "\n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "\n",
    "        self.memory.clear_memory()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CDPR4_env()\n",
    "batch_size = 64\n",
    "n_epochs = 4\n",
    "alpha = 1e-4\n",
    "agent = Agent(state_dim=env.observation_space.shape[0],\n",
    "              action_dim=env.action_space.shape[0], \n",
    "              batch_size=batch_size,\n",
    "              n_epochs=n_epochs,\n",
    "              policy_clip=0.2,\n",
    "              gamma=0.99,lamda=0.95, \n",
    "              adam_lr=alpha)\n",
    "n_games = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▎         | 11/300 [00:36<15:54,  3.30s/it, Episode Reward=-129243.98750]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     20\u001b[0m     agent\u001b[38;5;241m.\u001b[39mstore_data(current_state, action, prob, val, reward, done)\n\u001b[0;32m---> 21\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# TODO: check if need to learn each step?\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     current_state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     25\u001b[0m score_history\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "Cell \u001b[0;32mIn[6], line 104\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 104\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/cdpr_mujoco/lib/python3.9/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cdpr_mujoco/lib/python3.9/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cdpr_mujoco/lib/python3.9/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "best_score = -100\n",
    "score_history = []\n",
    "learn_iters = 0\n",
    "avg_score = 0\n",
    "\n",
    "pbar = tqdm(range(n_games), desc=\"Training Progress\")\n",
    "\n",
    "for i in pbar:\n",
    "    current_state,info = env.reset()\n",
    "    terminated,truncated = False,False\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action, prob, val = agent.choose_action(current_state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = 1 if (terminated or truncated) else 0\n",
    "        score += reward\n",
    "        agent.store_data(current_state, action, prob, val, reward, done)\n",
    "        agent.learn() # TODO: check if need to learn each step?\n",
    "        \n",
    "        current_state = next_state\n",
    "        \n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        \n",
    "    pbar.set_postfix({\n",
    "            'Episode Reward': f'{float(score):.5f}',\n",
    "        })\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdpr_mujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
