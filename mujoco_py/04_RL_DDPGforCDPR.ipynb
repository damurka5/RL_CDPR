{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from robot_model import *\n",
    "mps_device = torch.device(\"mps\")\n",
    "# Set up the environment\n",
    "# env = gym.make(\"Pendulum-v1\", render_mode=None)\n",
    "# num_states = env.observation_space.shape[0]\n",
    "# num_actions = env.action_space.shape[0]\n",
    "# upper_bound = env.action_space.high[0]\n",
    "# lower_bound = env.action_space.low[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Size of State Space ->  {}\".format(num_states))\n",
    "# print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "# print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "# print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "\n",
    "class CDPR4_env(CDPR4):\n",
    "    def __init__(self, start_state=torch.tensor([.0, .0, 1.0, .0, .0, .0]), desired_state=torch.tensor([.0, .0, 2.0, .0, .0, .0]), pos=torch.tensor([.0, .0, 1.0]), params=params, approx=1, mass=1):\n",
    "        super().__init__(pos=torch.tensor([.0, .0, 1.0]), params=params, approx=1, mass=1)\n",
    "\n",
    "        self.start_state = start_state  # start position 1m on Z\n",
    "        self.cur_state = torch.tensor([.0, .0, 1.0, .0, .0, .0]) # X=[pos, vel] in control\n",
    "        self.reward = -1e3 # reward 0 is 0 error in position, closer position to desired -> higher reward \n",
    "        self.desired_state = desired_state\n",
    "    \n",
    "        self.max_speed = 10\n",
    "        self.max_force = 45\n",
    "        \n",
    "        self.action_space = spaces.Box(\n",
    "            low=0, high=self.max_force, shape=(4,)\n",
    "        )\n",
    "        self.observation_space = spaces.Box(low=np.array([-1.154, -1.404, .0, -self.max_speed, -self.max_speed, -self.max_speed]), \n",
    "                                            high=np.array([1.154, 1.404, 3.220,  self.max_speed, self.max_speed, self.max_speed]))\n",
    "        \n",
    "    def reset(self):\n",
    "        state = self.start_state\n",
    "        self.reward = -1e3\n",
    "        \n",
    "        return state, {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        pos = self.cur_state[:3].flatten()\n",
    "        vel = self.cur_state[3:].flatten()\n",
    "        m = self.m\n",
    "\n",
    "        dt = self.dt\n",
    "        u = torch.clip(action, 0, self.max_force) # shape should be (4,1)\n",
    "        # print(f'pos {pos.shape}')\n",
    "        # print(f'self.desired_state[:3] {self.desired_state[:3].shape}')\n",
    "        costs = torch.linalg.norm(pos - self.desired_state[:3].flatten())**2 # reward function includes only position, no velocities\n",
    "\n",
    "        Jt_u = self.jacobian().T@u\n",
    "        new_vel = vel + Jt_u.flatten()*dt # vel + acc*dt, where acc = J.T@u\n",
    "        new_pos = pos + new_vel*dt\n",
    "\n",
    "        state = torch.hstack((new_pos, new_vel))\n",
    "        self.cur_state = state\n",
    "        \n",
    "        terminated = torch.allclose(self.cur_state, self.desired_state, atol=1e-03) # reached desired position\n",
    "        \n",
    "        \n",
    "        return state, -costs, terminated, False, {} #\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Box shape is inferred from low and high, expect their types to be np.ndarray, an integer or a float, actual type low: <class 'torch.Tensor'>, high: <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mCDPR4_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m num_states \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m num_actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36mCDPR4_env.__init__\u001b[0;34m(self, start_state, desired_state, pos, params, approx, mass)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_force \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m45\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space \u001b[38;5;241m=\u001b[39m spaces\u001b[38;5;241m.\u001b[39mBox(\n\u001b[1;32m     16\u001b[0m     low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_force, shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m,)\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;241m=\u001b[39m \u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1.154\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1.404\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_speed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_speed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_speed\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mhigh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.154\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.404\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3.220\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_speed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_speed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_speed\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cdpr_mujoco/lib/python3.9/site-packages/gymnasium/spaces/box.py:99\u001b[0m, in \u001b[0;36mBox.__init__\u001b[0;34m(self, low, high, shape, dtype, seed)\u001b[0m\n\u001b[1;32m     97\u001b[0m     shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m,)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBox shape is inferred from low and high, expect their types to be np.ndarray, an integer or a float, actual type low: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(low)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, high: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(high)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Capture the boundedness information before replacing np.inf with get_inf\u001b[39;00m\n\u001b[1;32m    104\u001b[0m _low \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(shape, low, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m is_float_integer(low) \u001b[38;5;28;01melse\u001b[39;00m low\n",
      "\u001b[0;31mValueError\u001b[0m: Box shape is inferred from low and high, expect their types to be np.ndarray, an integer or a float, actual type low: <class 'torch.Tensor'>, high: <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "env = CDPR4_env()\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     state = env.step(torch.tensor([20, 20, 20, 20], dtype=np.float32).reshape((4,1)))\n",
    "#     print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  6\n",
      "Size of Action Space ->  4\n",
      "Max Value of Action ->  45.0\n",
      "Min Value of Action ->  0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.max_action * self.net(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.action_net = nn.Sequential(\n",
    "            nn.Linear(action_dim, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.q_net = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_out = self.state_net(state)\n",
    "        action_out = self.action_net(action)\n",
    "        concat = torch.cat([state_out, action_out], dim=1)\n",
    "        return self.q_net(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000, batch_size=64):\n",
    "        self.capacity = capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self):\n",
    "        indices = torch.random.choice(len(self.buffer), self.batch_size)\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "\n",
    "        state, action, reward, next_state, done = map(torch.stack, zip(*batch))\n",
    "        return (\n",
    "            torch.FloatTensor(state),\n",
    "            torch.FloatTensor(action),\n",
    "            torch.FloatTensor(reward).unsqueeze(1),\n",
    "            torch.FloatTensor(next_state),\n",
    "            torch.FloatTensor(done).unsqueeze(1)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action)\n",
    "        self.actor.to(mps_device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action)\n",
    "        self.actor_target.to(mps_device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.critic.to(mps_device)\n",
    "        self.critic_target = Critic(state_dim, action_dim)\n",
    "        self.critic_target.to(mps_device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1))\n",
    "        return self.actor(state.to(mps_device)).flatten()\n",
    "\n",
    "    def train(self, gamma=0.99, tau=0.005):\n",
    "        if len(self.replay_buffer) < self.replay_buffer.batch_size:\n",
    "            return\n",
    "\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample()\n",
    "\n",
    "        # Compute the target Q value\n",
    "        target_Q = self.critic_target(next_state, self.actor_target(next_state))\n",
    "        target_Q = reward + (1 - done) * gamma * target_Q.detach()\n",
    "\n",
    "        # Get current Q estimate\n",
    "        current_Q = self.critic(state, action)\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss = nn.MSELoss()(current_Q, target_Q)\n",
    "\n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Compute actor loss\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "\n",
    "        # Optimize the actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update the frozen target models\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_ddpg(env, agent, num_episodes=100):\n",
    "    # ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(0.2) * np.ones(1))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset() \n",
    "        episode_reward = 0\n",
    "\n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            # action = action + ou_noise()\n",
    "            action = torch.clip(action, lower_bound, upper_bound)\n",
    "\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            agent.train()\n",
    "\n",
    "            if done or truncated:\n",
    "                print(f\"Episode {episode}: Reward = {episode_reward}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/cdpr_mujoco/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: clip() received an invalid combination of arguments - got (numpy.float32, numpy.float32, out=NoneType), but expected one of:\n * (Tensor min = None, Tensor max = None)\n * (Number min = None, Number max = None)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m max_action \u001b[38;5;241m=\u001b[39m upper_bound\n\u001b[1;32m      5\u001b[0m agent \u001b[38;5;241m=\u001b[39m DDPGAgent(state_dim, action_dim, max_action)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain_ddpg\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36mtrain_ddpg\u001b[0;34m(env, agent, num_episodes)\u001b[0m\n\u001b[1;32m     10\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# action = action + ou_noise()\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower_bound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupper_bound\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m next_state, reward, done, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     15\u001b[0m agent\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39mpush(state, action, reward, next_state, done)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/cdpr_mujoco/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2154\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2085\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_clip_dispatcher)\n\u001b[1;32m   2086\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip\u001b[39m(a, a_min, a_max, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2087\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2088\u001b[0m \u001b[38;5;124;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[1;32m   2089\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2152\u001b[0m \n\u001b[1;32m   2153\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cdpr_mujoco/lib/python3.9/site-packages/numpy/core/fromnumeric.py:66\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cdpr_mujoco/lib/python3.9/site-packages/numpy/core/fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m, method)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "File \u001b[0;32m~/miniconda3/envs/cdpr_mujoco/lib/python3.9/site-packages/torch/_tensor.py:1083\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert mps:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# Create and train the agent\n",
    "state_dim = num_states\n",
    "action_dim = num_actions\n",
    "max_action = upper_bound\n",
    "agent = DDPGAgent(state_dim, action_dim, max_action)\n",
    "train_ddpg(env, agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdpr_mujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
