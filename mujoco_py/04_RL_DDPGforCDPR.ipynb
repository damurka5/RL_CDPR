{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from robot_model import *\n",
    "# mps_device = torch.device(\"mps\")\n",
    "# Set up the environment\n",
    "# env = gym.make(\"Pendulum-v1\", render_mode=None)\n",
    "# num_states = env.observation_space.shape[0]\n",
    "# num_actions = env.action_space.shape[0]\n",
    "# upper_bound = env.action_space.high[0]\n",
    "# lower_bound = env.action_space.low[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Size of State Space ->  {}\".format(num_states))\n",
    "# print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "# print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "# print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "\n",
    "class CDPR4_env(CDPR4):\n",
    "    def __init__(self, start_state=np.array([.0, .0, 1.0, .0, .0, .0]), desired_state=np.array([.0, .0, 2.0, .0, .0, .0]), pos=np.array([.0, .0, 1.0]), params=params, approx=1, mass=1):\n",
    "        super().__init__(pos=np.array([.0, .0, 1.]), params=params, approx=1, mass=1)\n",
    "\n",
    "        self.start_state = start_state  # start position 1m on Z\n",
    "        self.cur_state = np.array([.0, .0, 1., .0, .0, .0]) # X=[pos, vel] in control\n",
    "        self.reward = -1e3 # reward 0 is 0 error in position, closer position to desired -> higher reward \n",
    "        self.desired_state = desired_state\n",
    "    \n",
    "        self.max_speed = 10\n",
    "        self.max_force = 45\n",
    "        \n",
    "        self.action_space = spaces.Box(\n",
    "            low=0.01, high=self.max_force, shape=(4,)\n",
    "        )\n",
    "        self.observation_space = spaces.Box(low=np.array([-1.154, -1.404, .0, -self.max_speed, -self.max_speed, -self.max_speed]), \n",
    "                                            high=np.array([1.154, 1.404, 3.220,  self.max_speed, self.max_speed, self.max_speed]))\n",
    "        \n",
    "    def reset(self):\n",
    "        state = self.start_state\n",
    "        self.reward = -1e3\n",
    "        \n",
    "        return state, {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        pos = self.cur_state[:3].flatten()\n",
    "        vel = self.cur_state[3:].flatten()\n",
    "        m = self.m\n",
    "\n",
    "        dt = self.dt\n",
    "        u = np.clip(action, 0.01, self.max_force) # shape should be (4,1)\n",
    "        # print(f'pos {pos.shape}')\n",
    "        # print(f'self.desired_state[:3] {self.desired_state[:3].shape}')\n",
    "        costs = np.linalg.norm(pos - self.desired_state[:3].flatten())**2 # reward function includes only position, no velocities\n",
    "        # print(f'cost {costs}')\n",
    "\n",
    "        Jt_u = self.jacobian().T@u # torch.from_numpy(self.jacobian().T@u.cpu().detach().numpy())\n",
    "        new_vel = vel + Jt_u.flatten()*dt # vel + acc*dt, where acc = J.T@u\n",
    "        new_pos = pos + new_vel*dt\n",
    "\n",
    "        state = np.hstack((new_pos, new_vel))\n",
    "        self.cur_state = state\n",
    "        \n",
    "        terminated = np.allclose(self.cur_state, self.desired_state, atol=1e-03) # reached desired position\n",
    "        \n",
    "        \n",
    "        return state, -costs, terminated, False, {} #\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damirnurtdinov/miniconda3/envs/cdpr_mujoco/lib/python3.9/site-packages/gymnasium/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "env = CDPR4_env()\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     state = env.step(np.array([20, 20, 20, 20], dtype=np.float32).reshape((4,1)))\n",
    "#     print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  6\n",
      "Size of Action Space ->  4\n",
      "Max Value of Action ->  45.0\n",
      "Min Value of Action ->  0.009999999776482582\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.max_action * self.net(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.action_net = nn.Sequential(\n",
    "            nn.Linear(action_dim, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.q_net = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_out = self.state_net(state)\n",
    "        action_out = self.action_net(action)\n",
    "        concat = torch.cat([state_out, action_out], dim=1)\n",
    "        return self.q_net(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000, batch_size=64):\n",
    "        self.capacity = capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self):\n",
    "        indices = np.random.choice(len(self.buffer), self.batch_size)\n",
    "        # batch = []\n",
    "        # for i in indices:\n",
    "        #     state, action, reward, next_state, done = self.buffer[i]\n",
    "        #     batch.append((state.float(), action, reward, next_state, done))\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "        # print(batch)\n",
    "\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return (\n",
    "            torch.FloatTensor(state),\n",
    "            torch.FloatTensor(action),\n",
    "            torch.FloatTensor(reward).unsqueeze(1),\n",
    "            torch.FloatTensor(next_state),\n",
    "            torch.FloatTensor(done).unsqueeze(1)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action)\n",
    "        # self.actor.to(mps_device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action)\n",
    "        # self.actor_target.to(mps_device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        # self.critic.to(mps_device)\n",
    "        self.critic_target = Critic(state_dim, action_dim)\n",
    "        # self.critic_target.to(mps_device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1))\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def train(self, gamma=0.99, tau=0.005):\n",
    "        if len(self.replay_buffer) < self.replay_buffer.batch_size:\n",
    "            return\n",
    "\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample()\n",
    "\n",
    "        # Compute the target Q value\n",
    "        target_Q = self.critic_target(next_state, self.actor_target(next_state))\n",
    "        target_Q = reward + (1 - done) * gamma * target_Q.detach()\n",
    "\n",
    "        # Get current Q estimate\n",
    "        current_Q = self.critic(state, action)\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss = nn.MSELoss()(current_Q, target_Q)\n",
    "\n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Compute actor loss\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "\n",
    "        # Optimize the actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update the frozen target models\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_ddpg(env, agent, num_episodes=100):\n",
    "    # ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(0.2) * np.ones(1))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset() \n",
    "        episode_reward = 0\n",
    "\n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            # action = action + ou_noise()\n",
    "            action = np.clip(action, lower_bound, upper_bound)\n",
    "\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            agent.train()\n",
    "\n",
    "            if done or truncated:\n",
    "                print(f\"Episode {episode}: Reward = {episode_reward}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m max_action \u001b[38;5;241m=\u001b[39m upper_bound\n\u001b[1;32m      5\u001b[0m agent \u001b[38;5;241m=\u001b[39m DDPGAgent(state_dim, action_dim, max_action)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain_ddpg\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m, in \u001b[0;36mtrain_ddpg\u001b[0;34m(env, agent, num_episodes)\u001b[0m\n\u001b[1;32m     17\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     18\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m---> 20\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Reward = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 50\u001b[0m, in \u001b[0;36mDDPGAgent.train\u001b[0;34m(self, gamma, tau)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Optimize the actor\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 50\u001b[0m \u001b[43mactor_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Update the frozen target models\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cdpr_mujoco/lib/python3.9/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cdpr_mujoco/lib/python3.9/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cdpr_mujoco/lib/python3.9/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create and train the agent\n",
    "state_dim = num_states\n",
    "action_dim = num_actions\n",
    "max_action = upper_bound\n",
    "agent = DDPGAgent(state_dim, action_dim, max_action)\n",
    "train_ddpg(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdpr_mujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
