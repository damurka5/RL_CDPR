{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from robot_model import *\n",
    "# mps_device = torch.device(\"mps\")\n",
    "# Set up the environment\n",
    "# env = gym.make(\"Pendulum-v1\", render_mode=None)\n",
    "# num_states = env.observation_space.shape[0]\n",
    "# num_actions = env.action_space.shape[0]\n",
    "# upper_bound = env.action_space.high[0]\n",
    "# lower_bound = env.action_space.low[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "\n",
    "class CDPR4_env(CDPR4):\n",
    "    def __init__(self, start_state=np.array([.0, .0, 1.0, .0, .0, .0]), desired_state=np.array([.0, .0, 2.0, .0, .0, .0]), pos=np.array([.0, .0, 1.0]), params=params, approx=1, mass=1):\n",
    "        super().__init__(pos=np.array([.0, .0, 1.]), params=params, approx=1, mass=1)\n",
    "\n",
    "        self.start_state = start_state  # start position 1m on Z\n",
    "        self.cur_state = np.array([.0, .0, 1., .0, .0, .0]) # X=[pos, vel] in control\n",
    "        self.reward = -1e3 # reward 0 is 0 error in position, closer position to desired -> higher reward \n",
    "        self.desired_state = desired_state\n",
    "    \n",
    "        self.max_speed = 10\n",
    "        self.max_force = 45\n",
    "        \n",
    "        self.action_space = spaces.Box(\n",
    "            low=0.01, high=self.max_force, shape=(4,)\n",
    "        )\n",
    "        self.observation_space = spaces.Box(low=np.array([-1.154, -1.404, .0, -self.max_speed, -self.max_speed, -self.max_speed]), \n",
    "                                            high=np.array([1.154, 1.404, 3.220,  self.max_speed, self.max_speed, self.max_speed]))\n",
    "        self._max_episode_steps = 1000 # default in gymnasium env is 1000\n",
    "        self._elapsed_steps = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        state = self.start_state\n",
    "        self.reward = -1e3\n",
    "        self._elapsed_steps = 0 \n",
    "        \n",
    "        return state, {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        pos = self.cur_state[:3].flatten()\n",
    "        vel = self.cur_state[3:].flatten()\n",
    "        m = self.m\n",
    "\n",
    "        dt = self.dt\n",
    "        u = np.clip(action, 0.01, self.max_force) # shape should be (4,1)\n",
    "        # print(f'pos {pos.shape}')\n",
    "        # print(f'self.desired_state[:3] {self.desired_state[:3].shape}')\n",
    "        costs = np.linalg.norm(pos - self.desired_state[:3].flatten()) # reward function includes only position, no velocities\n",
    "        # print(f'cost {costs}')\n",
    "\n",
    "        Jt_u = self.jacobian().T@u # torch.from_numpy(self.jacobian().T@u.cpu().detach().numpy())\n",
    "        new_vel = vel + (1/self.m)*Jt_u.flatten()*dt # vel + acc*dt, where acc = J.T@u\n",
    "        new_pos = pos + new_vel*dt\n",
    "\n",
    "        state = np.hstack((new_pos, new_vel))\n",
    "        self.cur_state = state\n",
    "        \n",
    "        terminated = np.allclose(self.cur_state, self.desired_state, atol=1e-03) # reached desired position\n",
    "        \n",
    "        self._elapsed_steps += 1\n",
    "\n",
    "        truncated = False\n",
    "        if self._elapsed_steps >= self._max_episode_steps:\n",
    "            truncated = True\n",
    "        \n",
    "        return state, -costs, terminated, truncated, {} #\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damirnurtdinov/miniconda3/envs/cdpr_mujoco/lib/python3.9/site-packages/gymnasium/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "env = CDPR4_env()\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     state = env.step(np.array([20, 20, 20, 20], dtype=np.float32).reshape((4,1)))\n",
    "#     print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  6\n",
      "Size of Action Space ->  4\n",
      "Max Value of Action ->  45.0\n",
      "Min Value of Action ->  0.009999999776482582\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.max_action * self.net(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.action_net = nn.Sequential(\n",
    "            nn.Linear(action_dim, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.q_net = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_out = self.state_net(state)\n",
    "        action_out = self.action_net(action)\n",
    "        concat = torch.cat([state_out, action_out], dim=1)\n",
    "        return self.q_net(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000, batch_size=64):\n",
    "        self.capacity = capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self):\n",
    "        indices = np.random.choice(len(self.buffer), self.batch_size)\n",
    "        # batch = []\n",
    "        # for i in indices:\n",
    "        #     state, action, reward, next_state, done = self.buffer[i]\n",
    "        #     batch.append((state.float(), action, reward, next_state, done))\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "        # print(batch)\n",
    "\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return (\n",
    "            torch.FloatTensor(state),\n",
    "            torch.FloatTensor(action),\n",
    "            torch.FloatTensor(reward).unsqueeze(1),\n",
    "            torch.FloatTensor(next_state),\n",
    "            torch.FloatTensor(done).unsqueeze(1)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action)\n",
    "        # self.actor.to(mps_device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action)\n",
    "        # self.actor_target.to(mps_device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-3)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        # self.critic.to(mps_device)\n",
    "        self.critic_target = Critic(state_dim, action_dim)\n",
    "        # self.critic_target.to(mps_device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=2e-3)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1))\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def train(self, gamma=0.99, tau=0.005):\n",
    "        if len(self.replay_buffer) < self.replay_buffer.batch_size:\n",
    "            return\n",
    "\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample()\n",
    "\n",
    "        # Compute the target Q value\n",
    "        target_Q = self.critic_target(next_state, self.actor_target(next_state))\n",
    "        target_Q = reward + (1 - done) * gamma * target_Q.detach()\n",
    "\n",
    "        # Get current Q estimate\n",
    "        current_Q = self.critic(state, action)\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss = nn.MSELoss()(current_Q, target_Q)\n",
    "\n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Compute actor loss\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "\n",
    "        # Optimize the actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update the frozen target models\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To store reward history of each episode\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "\n",
    "# Training loop\n",
    "def train_ddpg(env, agent, num_episodes=100):\n",
    "    # ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(0.2) * np.ones(1))\n",
    "\n",
    "    pbar = tqdm(range(num_episodes), desc=\"Training Progress\")\n",
    "\n",
    "    for episode in pbar:\n",
    "        prev_state, _ = env.reset() \n",
    "        episode_reward = 0\n",
    "\n",
    "        while True:\n",
    "            action = agent.select_action(prev_state)\n",
    "            # action = action + ou_noise()\n",
    "            action = np.clip(action, lower_bound, upper_bound)\n",
    "\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            agent.replay_buffer.push(prev_state, action, reward, next_state, done)\n",
    "\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            agent.train()\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "            \n",
    "            prev_state = state\n",
    "            \n",
    "        ep_reward_list.append(episode_reward)    \n",
    "        # print(f\"Episode {episode}: Reward = {episode_reward}\")\n",
    "        avg_reward = np.mean(ep_reward_list[-40:])\n",
    "        avg_reward_list.append(avg_reward)\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'Episode Reward': f'{episode_reward:.2f}',\n",
    "            'Avg Reward (last 40)': f'{avg_reward:.2f}'\n",
    "        })\n",
    "        \n",
    "    return ep_reward_list, avg_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 100/100 [06:49<00:00,  4.10s/it, Episode Reward=-38558117304.43, Avg Reward (last 40)=-30857547301.92]\n"
     ]
    }
   ],
   "source": [
    "# Create and train the agent\n",
    "state_dim = num_states\n",
    "action_dim = num_actions\n",
    "max_action = upper_bound\n",
    "agent = DDPGAgent(state_dim, action_dim, max_action)\n",
    "ep_rewards, avg_rewards = train_ddpg(env, agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdpr_mujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
