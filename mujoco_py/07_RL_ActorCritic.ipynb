{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "import gym\n",
    "import warnings\n",
    "from torch.distributions.categorical import Categorical\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from robot_model import *\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "\n",
    "class CDPR4_env(CDPR4):\n",
    "    def __init__(self, start_state=np.array([.0, .0, 1.0, .0, .0, .0]), desired_state=np.array([.0, .0, 2.0, .0, .0, .0]), pos=np.array([.0, .0, 1.0]), params=params, approx=1, mass=1):\n",
    "        super().__init__(pos=np.array([.0, .0, 1.]), params=params, approx=1, mass=1)\n",
    "\n",
    "        self.start_state = start_state.copy()  # start position 1m on Z\n",
    "        self.cur_state = np.array([.0, .0, 1., .0, .0, .0]) # X=[pos, vel] in control\n",
    "        self.reward = 0 # reward 0 is 0 error in position, closer position to desired -> higher reward \n",
    "        self.desired_state = desired_state\n",
    "        self.v = np.array([.0, .0, .0])\n",
    "        self.control = np.array([.0, .0, .0, .0])\n",
    "    \n",
    "        self.max_speed = 10\n",
    "        self.max_force = 15 \n",
    "        self.dt = 0.1\n",
    "        \n",
    "        self.action_space = spaces.Discrete(16) # [_,_,_,_] each place 0/1\n",
    "        self.action_decoder = {\n",
    "            i: np.array([int(b) for b in f'{i:04b}']) for i in range(16)\n",
    "        }\n",
    "        self.observation_space = spaces.Box(low=np.array([-1.154, -1.404, .0, -self.max_speed, -self.max_speed, -self.max_speed]), \n",
    "                                            high=np.array([1.154, 1.404, 3.220,  self.max_speed, self.max_speed, self.max_speed]))\n",
    "        self._max_episode_steps = 500 # default in gymnasium env is 1000\n",
    "        self._elapsed_steps = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        state = self.observation_space.sample()\n",
    "        self._elapsed_steps = 0\n",
    "        self.pos = state[:3].flatten()\n",
    "        self.v = state[3:].flatten()\n",
    "        \n",
    "        self.reward = -np.linalg.norm(self.pos - self.desired_state[:3].flatten())\n",
    "        \n",
    "        return state, {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        pos = self.cur_state[:3].flatten()\n",
    "        vel = self.cur_state[3:].flatten()\n",
    "        m = self.m\n",
    "\n",
    "        dt = self.dt\n",
    "        u = self.max_force*self.action_decoder[action].reshape((4,1))\n",
    "        costs = np.linalg.norm(pos - self.desired_state[:3].flatten())\n",
    "        costs += 0.5*np.linalg.norm(vel - self.desired_state[3:].flatten())\n",
    "        \n",
    "        dXdt = self.B() @ u + np.array([.0, .0, .0, .0, .0, -g]).reshape((6,1))\n",
    "        new_vel = vel + dXdt[:3].flatten()*dt\n",
    "        new_pos = pos + vel*dt + 0.5*dXdt[3:].flatten()*dt**2\n",
    "        self.pos = new_pos\n",
    "        self.v = new_vel\n",
    "\n",
    "        state = np.hstack((new_pos, new_vel))\n",
    "        self.cur_state = state\n",
    "        terminated = np.allclose(self.cur_state, self.desired_state, atol=1e-03) # reached desired position\n",
    "        self._elapsed_steps += 1\n",
    "        \n",
    "        truncated = False\n",
    "        if self._elapsed_steps >= self._max_episode_steps:\n",
    "            truncated = True\n",
    "        \n",
    "        return state, -costs, terminated, truncated, {} #\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CDPR4_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, fc1_dims=512, fc2_dims=512):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(n_observations, fc1_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc1_dims, fc2_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc2_dims, n_actions)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(n_observations, fc1_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc1_dims, fc2_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc2_dims, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        v = self.critic(state)\n",
    "        pi = F.softmax(self.actor(state), dim=-1)\n",
    "        return v, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "import torch.optim as optim\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, alpha=0.0003, gamma=0.99, n_observations=6, n_actions=16):\n",
    "        self.gamma = gamma\n",
    "        self.n_actions = n_actions\n",
    "        self.action = None\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "\n",
    "        self.actor_critic = ActorCriticNetwork(n_observations=n_observations, n_actions=n_actions)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=alpha)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = torch.tensor([observation], dtype=torch.float32)\n",
    "        _, probs = self.actor_critic(state)\n",
    "\n",
    "        action_probabilities = Categorical(probs)\n",
    "        action = action_probabilities.sample()\n",
    "        log_prob = action_probabilities.log_prob(action)\n",
    "        self.action = action\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def learn(self, state, reward, state_, done):\n",
    "        state = torch.tensor([state], dtype=torch.float32)\n",
    "        state_ = torch.tensor([state_], dtype=torch.float32)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        state_value, probs = self.actor_critic(state)\n",
    "        state_value_, _ = self.actor_critic(state_)\n",
    "        state_value = torch.squeeze(state_value)\n",
    "        state_value_ = torch.squeeze(state_value_)\n",
    "\n",
    "        action_probs = Categorical(probs)\n",
    "        log_prob = action_probs.log_prob(self.action)\n",
    "\n",
    "        delta = reward + self.gamma * state_value_ * (1 - int(done)) - state_value\n",
    "        actor_loss = -log_prob * delta.detach()\n",
    "        critic_loss = delta.pow(2)\n",
    "        total_loss = actor_loss + critic_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = Agent(alpha=1e-5, n_observations=env.observation_space.shape[0], n_actions=env.action_space.n)\n",
    "# agent.actor_critic.load_state_dict(torch.load('models/07_ActorCritic1800', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "agent = Agent(alpha=5e-4, n_observations=env.observation_space.shape[0], n_actions=env.action_space.n)\n",
    "n_games = 1800\n",
    "\n",
    "best_score = -90\n",
    "score_history = []\n",
    "\n",
    "pbar = tqdm(range(n_games), desc=\"Training Progress\")\n",
    "\n",
    "for i in pbar:\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    trunc = False\n",
    "    score = 0\n",
    "    while not (done or trunc):\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, trunc, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.learn(observation, reward, observation_, done)\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        torch.save(agent.actor_critic.state_dict(), f'models/07_ActorCritic_separate_linearReward_21_11_24_reward{int(score)}')\n",
    "        \n",
    "    pbar.set_postfix({\n",
    "            'Episode Reward': f'{float(score):.5f}',\n",
    "            'avg_score': f'{float(avg_score):.5f}',\n",
    "            'pos':f'{env.pos}'\n",
    "            \n",
    "        })\n",
    "    # print('episode ', i, 'score %.1f' % score, 'avg_score %.1f' % avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1800), score_history[-1800:])\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(agent.actor_critic.state_dict(), 'models/07_ActorCritic_separate_squaredReward_21_11_24_reward_average')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-130.12593631482056"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(score_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdpr_mujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
