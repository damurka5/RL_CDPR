{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO, DDPG, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import (\n",
    "    DummyVecEnv,\n",
    "    VecVideoRecorder,\n",
    "    SubprocVecEnv,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import os\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from typing import Callable, Dict, List\n",
    "import torch\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "import wandb\n",
    "import types\n",
    "\n",
    "from robot_model import CDPR4_env, MAX_EPISODE_STEPS\n",
    "\n",
    "import sys\n",
    "# sys.setrecursionlimit(9_999_999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(\n",
    "    initial_value: float, min_value_percent: float = 0.1\n",
    ") -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule that decreases from initial_value to\n",
    "    (initial_value * min_value_percent).\n",
    "\n",
    "    :param initial_value: Initial learning rate\n",
    "    :param min_value_percent: Final learning rate as a percentage of initial value (0.1 = 10%)\n",
    "    :return: schedule that computes current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "        Learning rate will decrease from initial_value to (initial_value * min_value_percent)\n",
    "\n",
    "        :param progress_remaining: goes from 1 (beginning) to 0 (end)\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        min_value = initial_value * min_value_percent\n",
    "        return min_value + (initial_value - min_value) * progress_remaining\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def cosine_schedule_with_warmup(\n",
    "    initial_value: float,\n",
    "    warmup_fraction: float = 0.03,\n",
    "    min_value_percent: float = 0.1,\n",
    ") -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Cosine learning rate schedule with warmup that decreases from initial_value to\n",
    "    (initial_value * min_value_percent).\n",
    "\n",
    "    :param initial_value: Initial learning rate after warmup\n",
    "    :param warmup_fraction: Fraction of total steps to use for warmup (0.1 = 10%)\n",
    "    :param min_value_percent: Final learning rate as a percentage of initial value (0.1 = 10%)\n",
    "    :return: schedule that computes current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "        Learning rate will follow:\n",
    "        1. Linear warmup from 0 to initial_value for warmup_fraction of total steps\n",
    "        2. Cosine decay from initial_value to (initial_value * min_value_percent)\n",
    "\n",
    "        :param progress_remaining: goes from 1 (beginning) to 0 (end)\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        progress_done = 1 - progress_remaining\n",
    "        min_value = initial_value * min_value_percent\n",
    "\n",
    "        # Handle warmup phase\n",
    "        if progress_done < warmup_fraction:\n",
    "            # Linear warmup\n",
    "            return initial_value * (progress_done / warmup_fraction)\n",
    "\n",
    "        # Handle cosine decay phase\n",
    "        progress_after_warmup = (progress_done - warmup_fraction) / (\n",
    "            1 - warmup_fraction\n",
    "        )\n",
    "        cosine_decay = 0.5 * (1 + np.cos(np.pi * progress_after_warmup))\n",
    "\n",
    "        # Interpolate between initial_value and min_value using cosine curve\n",
    "        return min_value + (initial_value - min_value) * cosine_decay\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def get_images(self):\n",
    "    self.remotes[0].send((\"render\", None))\n",
    "    return [self.remotes[0].recv()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(is_continuous=False):\n",
    "    env = Monitor(CDPR4_env(is_continuous=is_continuous))\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_PPO(\n",
    "    activation_fn: str = \"ReLU\",\n",
    "    net_arch: Dict[str, List[int]] = dict(\n",
    "        pi=[512, 512],\n",
    "        vf=[512, 512],\n",
    "    ),\n",
    "    lr_initial_value: float = 8e-4,\n",
    "    warmup_fraction: float = 0.01,\n",
    "    min_value_percent: float = 0.1,\n",
    "    batch_size: int = 32,\n",
    "    n_epochs: int = 5,\n",
    "    gamma: float = 0.99,\n",
    "    gae_lambda: float = 0.95,\n",
    "    clip_range: float = 0.2,\n",
    "    max_grad_norm: float = 100.0,\n",
    "    use_sde: bool = False,\n",
    "    sde_sample_freq: int = -1,\n",
    "    ent_coef: float = 0.0005,\n",
    "    # Training parameters\n",
    "    total_timesteps: int = 1_000_000,\n",
    "    log_interval: int = 1,\n",
    "    progress_bar: bool = True,\n",
    "    record_video: bool = False,\n",
    "    device: str = \"cpu\",\n",
    "):\n",
    "    activation_fn_map = {\n",
    "        \"ReLU\": torch.nn.ReLU,\n",
    "        \"Tanh\": torch.nn.Tanh,\n",
    "        \"LeakyReLU\": torch.nn.LeakyReLU,\n",
    "    }\n",
    "    activation_fn = activation_fn_map[activation_fn]\n",
    "\n",
    "    env = make_vec_env(\n",
    "        lambda: make_env(),\n",
    "        n_envs=int(4),\n",
    "        vec_env_cls=SubprocVecEnv,\n",
    "    )\n",
    "    env.get_images = types.MethodType(get_images, env)\n",
    "    if record_video:\n",
    "        env = VecVideoRecorder(\n",
    "            env,\n",
    "            f\"videos/{wandb.run.id}\",\n",
    "            record_video_trigger=lambda x: x % 100 == 0,\n",
    "            video_length=MAX_EPISODE_STEPS * 3,\n",
    "        )\n",
    "\n",
    "    policy_kwargs = dict(\n",
    "        log_std_init=-2.0,\n",
    "        ortho_init=False,\n",
    "        activation_fn=activation_fn,\n",
    "        net_arch=net_arch,\n",
    "    )\n",
    "\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",  # Multi-Layer Perceptron policy\n",
    "        env,\n",
    "        learning_rate=cosine_schedule_with_warmup(\n",
    "            initial_value=lr_initial_value,\n",
    "            warmup_fraction=warmup_fraction,\n",
    "            min_value_percent=min_value_percent,\n",
    "        ),\n",
    "        n_steps=MAX_EPISODE_STEPS,  # Number of steps to run for each environment per update\n",
    "        batch_size=batch_size,  # Minibatch size\n",
    "        n_epochs=n_epochs,\n",
    "        gamma=gamma,  # Discount factor\n",
    "        gae_lambda=gae_lambda,  # GAE parameter\n",
    "        clip_range=clip_range,  # Clipping parameter\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        use_sde=use_sde,\n",
    "        sde_sample_freq=sde_sample_freq,\n",
    "        ent_coef=ent_coef,\n",
    "        verbose=1,\n",
    "        tensorboard_log=\"./ppo_cdpr4_tensorboard/\",\n",
    "        device=device,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "    )\n",
    "\n",
    "    model.learn(\n",
    "        total_timesteps=total_timesteps,\n",
    "        log_interval=log_interval,\n",
    "        progress_bar=progress_bar,\n",
    "        callback=WandbCallback(\n",
    "            verbose=2,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model_DDPG(\n",
    "    activation_fn: str = \"ReLU\",\n",
    "    net_arch: Dict[str, List[int]] = dict(\n",
    "        pi=[512, 512],\n",
    "        qf=[512, 512],\n",
    "    ),\n",
    "    lr_initial_value: float = 8e-4,\n",
    "    warmup_fraction: float = 0.01,\n",
    "    min_value_percent: float = 0.1,\n",
    "    \n",
    "    # Training parameters\n",
    "    buffer_size: int = 1_000_000,\n",
    "    total_timesteps: int = 1_000_000,\n",
    "    batch_size: int = 256,\n",
    "    tau: float = 0.005,\n",
    "    gamma: float = 0.99,\n",
    "    action_noise = None,\n",
    "    learning_starts: int = 100,\n",
    "    \n",
    "    log_interval: int = 1,\n",
    "    progress_bar: bool = True,\n",
    "    record_video: bool = False,\n",
    "    device: str = \"cpu\",\n",
    "):\n",
    "    activation_fn_map = {\n",
    "        \"ReLU\": torch.nn.ReLU,\n",
    "        \"Tanh\": torch.nn.Tanh,\n",
    "        \"LeakyReLU\": torch.nn.LeakyReLU,\n",
    "    }\n",
    "    activation_fn = activation_fn_map[activation_fn]\n",
    "\n",
    "    env = make_vec_env(\n",
    "        lambda: make_env(is_continuous=True),\n",
    "        n_envs=int(4),\n",
    "        vec_env_cls=SubprocVecEnv,\n",
    "    )\n",
    "    env.get_images = types.MethodType(get_images, env)\n",
    "    if record_video:\n",
    "        env = VecVideoRecorder(\n",
    "            env,\n",
    "            f\"videos/{wandb.run.id}\",\n",
    "            record_video_trigger=lambda x: x % 100 == 0,\n",
    "            video_length=MAX_EPISODE_STEPS * 3,\n",
    "        )\n",
    "\n",
    "    policy_kwargs = dict(\n",
    "        activation_fn=activation_fn,\n",
    "        # net_arch=net_arch,\n",
    "    )\n",
    "\n",
    "    model = TD3(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        learning_rate=cosine_schedule_with_warmup(\n",
    "            initial_value=lr_initial_value,\n",
    "            warmup_fraction=warmup_fraction,\n",
    "            min_value_percent=min_value_percent,\n",
    "        ),\n",
    "        learning_starts=learning_starts,\n",
    "        buffer_size=buffer_size,\n",
    "        batch_size=batch_size,\n",
    "        tau=tau,\n",
    "        gamma=gamma,\n",
    "        action_noise=action_noise,\n",
    "        \n",
    "        device=device,\n",
    "        tensorboard_log=\"./ddpg_cdpr4_tensorboard/\",\n",
    "        # policy_kwargs=policy_kwargs,\n",
    "    )\n",
    "\n",
    "    model.learn(\n",
    "        total_timesteps=total_timesteps,\n",
    "        log_interval=log_interval,\n",
    "        progress_bar=progress_bar,\n",
    "        callback=WandbCallback(\n",
    "            verbose=2,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_PPO():\n",
    "    run = wandb.init(\n",
    "        project=\"sb3\",\n",
    "        sync_tensorboard=True,\n",
    "        monitor_gym=True,\n",
    "    )\n",
    "    model = train_model_PPO(\n",
    "        record_video=False,\n",
    "        total_timesteps=1_000_000,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "    # Save the trained model\n",
    "    save_path = f\"models/{wandb.run.id}/ppo_cdpr4_model\"\n",
    "    model.save(save_path)\n",
    "    print(f\"Model Saved as '{save_path}.zip'.\")\n",
    "\n",
    "    # Directory to save GIFs\n",
    "    gif_dir = f\"cdpr4_gifs/{wandb.run.id}\"\n",
    "    os.makedirs(gif_dir, exist_ok=True)\n",
    "\n",
    "    # Testing parameters\n",
    "    num_test_episodes = 5\n",
    "    max_test_steps = MAX_EPISODE_STEPS  # Adjusted for visualization\n",
    "    render_every_n = 1  # Render every 1 episode\n",
    "\n",
    "    env = make_env()\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    # Test the model with a few episodes and create GIFs\n",
    "    print(\"Starting Testing Episodes with Visualization...\")\n",
    "    for episode in range(num_test_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        frames = []  # List to store frames for the GIF\n",
    "        step_count = 0  # Initialize step counter\n",
    "\n",
    "        while not done and step_count < max_test_steps:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            step_result = env.step(action)\n",
    "            obs, reward, done, info = step_result\n",
    "            total_reward += reward[0]\n",
    "            done = done[0]\n",
    "            if step_count % render_every_n == 0 or done:\n",
    "                frame = env.envs[0].env.render(reward=reward[0], mode=\"rgb_array\")\n",
    "                frames.append(frame)\n",
    "            step_count += 1\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode + 1}: Total Reward = {total_reward}, Done: {done}, Steps: {step_count}\"\n",
    "        )\n",
    "        print(f\"Final observation: {obs}\")\n",
    "\n",
    "        gif_path = os.path.join(gif_dir, f\"episode_{episode + 1}.gif\")\n",
    "        imageio.mimsave(gif_path, frames, fps=1)\n",
    "        print(f\"Saved GIF to {gif_path}\")\n",
    "\n",
    "def train_single_DDPG():\n",
    "    run = wandb.init(\n",
    "        project=\"sb3\",\n",
    "        sync_tensorboard=True,\n",
    "        monitor_gym=True,\n",
    "    )\n",
    "    model = train_model_DDPG(\n",
    "        record_video=False,\n",
    "        total_timesteps=1_000_000,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "    # Save the trained model\n",
    "    save_path = f\"models/{wandb.run.id}/ddpg_cdpr4_model\"\n",
    "    model.save(save_path)\n",
    "    print(f\"Model Saved as '{save_path}.zip'.\")\n",
    "\n",
    "    # Directory to save GIFs\n",
    "    gif_dir = f\"cdpr4_gifs/ddpg/{wandb.run.id}\"\n",
    "    os.makedirs(gif_dir, exist_ok=True)\n",
    "\n",
    "    # Testing parameters\n",
    "    num_test_episodes = 5\n",
    "    max_test_steps = MAX_EPISODE_STEPS  # Adjusted for visualization\n",
    "    render_every_n = 1  # Render every 1 episode\n",
    "\n",
    "    env = make_env()\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    # Test the model with a few episodes and create GIFs\n",
    "    print(\"Starting Testing Episodes with Visualization...\")\n",
    "    for episode in range(num_test_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        frames = []  # List to store frames for the GIF\n",
    "        step_count = 0  # Initialize step counter\n",
    "\n",
    "        while not done and step_count < max_test_steps:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            step_result = env.step(action)\n",
    "            obs, reward, done, info = step_result\n",
    "            total_reward += reward[0]\n",
    "            done = done[0]\n",
    "            if step_count % render_every_n == 0 or done:\n",
    "                frame = env.envs[0].env.render(reward=reward[0], mode=\"rgb_array\")\n",
    "                frames.append(frame)\n",
    "            step_count += 1\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode + 1}: Total Reward = {total_reward}, Done: {done}, Steps: {step_count}\"\n",
    "        )\n",
    "        print(f\"Final observation: {obs}\")\n",
    "\n",
    "        gif_path = os.path.join(gif_dir, f\"episode_{episode + 1}.gif\")\n",
    "        imageio.mimsave(gif_path, frames, fps=1)\n",
    "        print(f\"Saved GIF to {gif_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdamurka\u001b[0m (\u001b[33mdamurka-innopolis-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/damirnurtdinov/Desktop/My Courses/Диплом/RL_CDPR_MuJoCo/mujoco_py/wandb/run-20241205_111231-lmpyrosa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/damurka-innopolis-university/sb3/runs/lmpyrosa' target=\"_blank\">wobbly-wood-25</a></strong> to <a href='https://wandb.ai/damurka-innopolis-university/sb3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/damurka-innopolis-university/sb3' target=\"_blank\">https://wandb.ai/damurka-innopolis-university/sb3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/damurka-innopolis-university/sb3/runs/lmpyrosa' target=\"_blank\">https://wandb.ai/damurka-innopolis-university/sb3/runs/lmpyrosa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_single_DDPG()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdpr_mujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
